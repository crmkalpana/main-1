#Data Engineering<br>Synapse Data Engineering empowers data engineers to be able to transform their data at scale using Spark and build out their lakehouse architecture. There are five key areas of Synapse Data Engineering that you can start using for your projects today:

Lakehouse for all your organizational data: The lakehouse is a new item in Fabric that combines the best of the data lake and the data warehouse in a single experience. It enables users to ingest, prepare, and share organizational data in an open format in the lake. Later you can access it through multiple engines such as Spark, T-SQL, and Power BI. It provides various data integration options such as dataflows and pipelines, shortcuts to external data sources, and data product sharing capabilities.

Performant Spark engine & runtime: Synapse Data engineering provides customers with an optimized Spark runtime with the latest versions of Spark (3.3.1), Delta (2.2), and Python (3.10). It uses Delta Lake as the common table format for all engines, enabling easy data sharing and reporting with no data movement. The runtime comes with Spark optimizations, enhancing your query performance without any configurations. It also offers starter pools and high-concurrency mode to speed up and reuse your Spark sessions, saving you time and cost.

Spark Admin & configurations: Workspace admins with appropriate permissions can create and configure custom pools to optimize the performance and cost of their Spark workloads. They can also install libraries, select the runtime version, and set Spark properties to customize the Spark environment. These settings will apply to all notebooks and Spark jobs in the workspace, unless otherwise specified.

Developer Experience: Developers can use notebooks, Spark jobs, or their preferred IDE to author and execute Spark code in Fabric. They can natively access the lakehouse data, collaborate with others, install libraries, track history, do in-line monitoring, and get recommendations from the Spark advisor. They can also use Data Wrangler to easily prepare data with a low-code UI.

Platform Integration: All Synapse data engineering items, including notebooks, Spark jobs, pipelines, and lakehouses, are integrated deeply into the Fabric platform (enterprise information management capabilities, lineage, sensitivity labels, and endorsements). This integration will continue to deepen this semester with many new investments.<br>|  **Feature**  |<br>|:------------|<br>|Schema support for Lakehouse   |<br><div data-wrapper="true" dir="ltr" style="font-size:9pt;font-family:'Segoe UI','Helvetica Neue',sans-serif;"><div><span style="background-color:#ffffff; color:#161616; display:inline !important; float:none; font-family:&quot;Segoe UI&quot;,SegoeUI,&quot;Helvetica Neue&quot;,Helvetica,Arial,sans-serif; font-size:16px; font-style:normal; font-variant-caps:normal; font-weight:400; letter-spacing:normal; text-align:start; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-indent:0px; text-transform:none; white-space:normal; word-spacing:0px">The lakehouse will support 3-part naming convention. It enables you to add schemas to your lakehouses, which is consistent with the current warehouse experience.</span></div></div><br>#Data Science<br>Synapse Data Science provides data scientists with an end-to-end workflow for building their machine learning models, from exploration to model scoring. From a data exploration perspective, data scientists can use R and Python in notebooks, and built-in tools like Data Wrangler for easy analysis. Users can track and compare their model experiments and runs with MLFlow. They can save the best performing model in the workspace as a new model item and easily use Predict for batch scoring at scale. Data science in Fabric is deeply integrated with the rest of the stack, meaning it's seamless to score data in a lakehouse, write back the predictions to OneLake, and visualize the data in reports using Direct Lake mode<br>|  **Feature**  |<br>|:------------|<br>|Schema support for Lakehouse   ||  **Feature**  |<br>|:------------|<br>|Semantic Link GA   |<br><div data-wrapper="true" dir="ltr" style="font-size:9pt;font-family:'Segoe UI','Helvetica Neue',sans-serif;"><div><span style="background-color:#ffffff; color:#161616; display:inline !important; float:none; font-family:&quot;Segoe UI&quot;,SegoeUI,&quot;Helvetica Neue&quot;,Helvetica,Arial,sans-serif; font-size:16px; font-style:normal; font-variant-caps:normal; font-weight:400; letter-spacing:normal; text-align:start; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-indent:0px; text-transform:none; white-space:normal; word-spacing:0px">The lakehouse will support 3-part naming convention. It enables you to add schemas to your lakehouses, which is consistent with the current warehouse experience.</span></div></div><br><br><div data-wrapper="true" dir="ltr" style="font-size:9pt;font-family:'Segoe UI','Helvetica Neue',sans-serif;"><div><span style="background-color:#ffffff; color:#161616; display:inline !important; float:none; font-family:&quot;Segoe UI&quot;,SegoeUI,&quot;Helvetica Neue&quot;,Helvetica,Arial,sans-serif; font-size:16px; font-style:normal; font-variant-caps:normal; font-weight:400; letter-spacing:normal; text-align:start; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-indent:0px; text-transform:none; white-space:normal; word-spacing:0px">Semantic link bridges the gap between data science and BI by providing a Python library (SemPy) that enables data scientists to interact with Power BI datasets and measures. You can use SemPy to read, explore, query, and validate data in Power BI from Python notebooks, and use the library's features to detect and resolve data challenges. Users can also write back to the Power BI dataset through the lakehouse with Direct Lake mode.</span></div></div><br>